{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music VAE 구현\n",
    "\n",
    "1. VAE - 의미론적으로 의미 있는 data의 잠재적 표현을 생성하는 모델\n",
    "    - 전통적 LSTM은 후방 붕괴 문제로 music data와 같은 긴 sequence의 디코딩이 어려움\n",
    "    - 후방붕괴 : sequence가 생성됨에 따라 잠재 상태의 소멸 영향\n",
    "    - hierarchical decoder(계층적 디코더)를 통해 sampling 된 latent vector(잠재 백터)는 flat decoder가 아닌 여러 레벨의 decoder 통과\n",
    "\n",
    "2. data set & preprocess\n",
    "    - Music VAE Model은 midi 확장자의 data 사용\n",
    "    - 양방향 LSTM encoder와 계층적 단방향 LSTM decoder 사용 \n",
    "    - Music VAE 구현을 위해 midi format을 tfrecord format으로 변환 필요\n",
    "        - tfrecord : tensorflow의 학습 데이터 저장을 위한 binary data format\n",
    "        - midi -> tfrecord format transform은 magenta library 활용해 디렉토리 자체의 변환 가능(convert_dir_to_note_sequences)\n",
    "        - 변환 된 tfrecord 파일 저장\n",
    "\n",
    "3. Training\n",
    "    - tfrecord data를 입력 sequence로 VAE Model train\n",
    "    - Bidirectional Encoder : 2개층의 양방향 LSTM으로 sequence 정보를 갖도록 함\n",
    "    - Hierarchical Decoder : 계층적 단방향 LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music VAE\n",
    "\n",
    "1. 1개의 midi data에서 drum sample 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "c:\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# 필요 모듈 import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import IPython\n",
    "import collections\n",
    "import note_seq\n",
    "\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.contrib import training as contrib_training\n",
    "from magenta.models.music_vae.base_model import MusicVAE\n",
    "from magenta.models.music_vae import data_hierarchical\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae import data\n",
    "from magenta.scripts.convert_dir_to_note_sequences import convert_directory # tfrecord 변환\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel # 훈련 모델\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tf_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "\n",
    "data_root= 'D:/PozaLabs_test/VAE/midi_data/groove' # data 저장 경로\n",
    "rec_root= 'D:/PozaLabs_test/VAE/midi_data/m.tfrecord'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converting files in 'D:/PozaLabs_test/VAE/midi_data/groove\\'.\n",
      "INFO:tensorflow:0 files converted.\n",
      "INFO:tensorflow:Converted MIDI file D:/PozaLabs_test/VAE/midi_data/groove\\1_funk_80_beat_4-4.mid.\n"
     ]
    }
   ],
   "source": [
    "# midi format data가 저장된 directory 자체의 format 변환\n",
    "# magenta library 활용\n",
    "\n",
    "convert_directory(data_root,rec_root,recursive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magenta github code - configs.py 참조\n",
    "\n",
    "class Config(collections.namedtuple('Config', ['model', 'hparams', 'note_sequence_augmenter', 'data_converter', 'train_examples_path', 'eval_example_path', 'tfds_name'])):\n",
    "    def values(self):\n",
    "        return self._asdict()\n",
    "\n",
    "Config.__new__.__defaults__= (None,) * len(Config._fields)\n",
    "\n",
    "CONFIG_MAP = {}\n",
    "\n",
    "HParams = contrib_training.HParams\n",
    "\n",
    "# groovae config 활용\n",
    "# drum\n",
    "\n",
    "CONFIG_MAP['drum_4bar'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(), # BidirectionalLstmEncoder\n",
    "                   lstm_models.GrooveLstmDecoder()), # Hierarchical Decoder\n",
    "\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size= 512, # 데이터 배치사이즈\n",
    "            max_seq_len= 64,  # 4마디 길이지정, 16이 1마디, sequences of 64th note events\n",
    "            z_size= 512, # latent vector(잠재백터)\n",
    "            enc_rnn_size= [2048], # 2048개의 은닉층을 갖는 양방향 LSTM 레이어 1개를 갖는 인코더\n",
    "            dec_rnn_size= [1024, 1024], # 1024개의 은닉층을 갖는 계층적 단방향 LSTM 레이어 2개를 갖는 디코더\n",
    "            max_beta= 0.2,\n",
    "            free_bits= 48\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "\n",
    "    # 4마디 단위로 sequence 분리\n",
    "    data_converter=data.GrooveConverter(\n",
    "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4, max_tensors_per_notesequence=20,\n",
    "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
    "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
    "    # tfds_name='C:/Users/SESE/iCloudDrive/POZA/VAE/groove/4bar-midionly'\n",
    "    train_examples_path='D:/PozaLabs_test/VAE/midi_data/m.tfrecord' # 데이터 경로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magenta github code - train.py 참조\n",
    "# train code source code\n",
    "\n",
    "def _trial_summary(hparams, examples_path, output_dir):\n",
    "    \"\"\"tensorboard summary text\"\"\"\n",
    "\n",
    "    examples_path_summary = tf.summary.text('examples_path', tf.constant(examples_path, name='examples_path'), collections=[])\n",
    "\n",
    "    hparams_dict = hparams.values()\n",
    "\n",
    "    \"\"\"Hyper Parameter\"\"\"\n",
    "    # Create a markdown table from hparams.\n",
    "    header= '| Key | Value |\\n| :--- | :--- |\\n'\n",
    "    keys= sorted(hparams_dict.keys())\n",
    "    lines= ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
    "    hparams_table = header + '\\n'.join(lines) + '\\n'\n",
    "\n",
    "    hparam_summary = tf.summary.text('hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
    "        writer.add_summary(examples_path_summary.eval())\n",
    "        writer.add_summary(hparam_summary.eval())\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def _get_input_tensors(dataset, config):\n",
    "\n",
    "    \"\"\"dataset으로부터 tensor input\"\"\"\n",
    "    batch_size = config.hparams.batch_size\n",
    "    iterator = tf.data.make_one_shot_iterator(dataset)\n",
    "    (input_sequence, output_sequence, control_sequence, sequence_length)= iterator.get_next()\n",
    "    input_sequence.set_shape([batch_size, None, config.data_converter.input_depth])\n",
    "    output_sequence.set_shape([batch_size, None, config.data_converter.output_depth])\n",
    "    \n",
    "    if not config.data_converter.control_depth:\n",
    "        control_sequence = None\n",
    "    \n",
    "    else:\n",
    "        control_sequence.set_shape([batch_size, None, config.data_converter.control_depth])\n",
    "\n",
    "    sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
    "        \n",
    "    return {\n",
    "        'input_sequence': input_sequence,\n",
    "        'output_sequence': output_sequence,\n",
    "        'control_sequence': control_sequence,\n",
    "        'sequence_length': sequence_length\n",
    "    }\n",
    "\n",
    "\"\"\"train param & time setting\"\"\"\n",
    "def train(train_dir,\n",
    "          config,\n",
    "          dataset_fn,\n",
    "          checkpoints_to_keep= 5,\n",
    "          keep_checkpoint_every_n_hours= 1,\n",
    "          num_steps= None,\n",
    "          master= '',\n",
    "          num_sync_workers= 0,\n",
    "          num_ps_tasks= 0,\n",
    "          task= 0):\n",
    "          \n",
    "     \"\"\"train loop\"\"\"\n",
    "     tf.gfile.MakeDirs(train_dir)\n",
    "     is_chief= (task== 0)\n",
    "\n",
    "     with tf.Graph().as_default():\n",
    "         with tf.device(tf.train.replica_device_setter(\n",
    "            num_ps_tasks, merge_devices=True)):\n",
    "            \n",
    "             model = config.model\n",
    "             model.build(config.hparams,\n",
    "                         config.data_converter.output_depth,\n",
    "                         is_training=True)\n",
    "\n",
    "             \"\"\"Optimizer\"\"\"\n",
    "             optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
    "\n",
    "             hooks = []\n",
    "             if num_sync_workers:\n",
    "                optimizer = tf.train.SyncReplicasOptimizer(optimizer,num_sync_workers)\n",
    "                hooks.append(optimizer.make_session_run_hook(is_chief))\n",
    "\n",
    "             grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
    "             global_norm = tf.global_norm(grads)\n",
    "             tf.summary.scalar('global_norm', global_norm)\n",
    "            \n",
    "             if config.hparams.clip_mode == 'value':\n",
    "                g = config.hparams.grad_clip\n",
    "                clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
    "             elif config.hparams.clip_mode == 'global_norm':\n",
    "                clipped_grads = tf.cond(\n",
    "                    global_norm < config.hparams.grad_norm_clip_to_zero,\n",
    "                    lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
    "                        grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
    "                    lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
    "             else:\n",
    "                raise ValueError(\n",
    "                    'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
    "             train_op = optimizer.apply_gradients(\n",
    "                       list(zip(clipped_grads, var_list)),\n",
    "                       global_step=model.global_step,\n",
    "                       name='train_step')\n",
    "\n",
    "             logging_dict = {'global_step': model.global_step,\n",
    "                            'loss': model.loss}\n",
    "            \n",
    "             hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
    "             if num_steps:\n",
    "                hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
    "                \n",
    "             scaffold = tf.train.Scaffold(\n",
    "                saver=tf.train.Saver(\n",
    "                    max_to_keep=checkpoints_to_keep,\n",
    "                    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
    "            \n",
    "             tf_slim.training.train(\n",
    "                train_op=train_op,\n",
    "                logdir=train_dir,\n",
    "                scaffold=scaffold,\n",
    "                hooks=hooks,\n",
    "                save_checkpoint_secs= 60, # 저장 주기 시간 체크\n",
    "                master=master,\n",
    "                is_chief=is_chief)\n",
    "\n",
    "def evaluate(train_dir,\n",
    "             eval_dir,\n",
    "             config,\n",
    "             dataset_fn,\n",
    "             num_batches,\n",
    "             master=''):\n",
    "\n",
    "     \"\"\"Evaluate the model repeatedly.\"\"\"\n",
    "     tf.gfile.MakeDirs(eval_dir)\n",
    "\n",
    "     _trial_summary(config.hparams, config.eval_examples_path or config.tfds_name, eval_dir)\n",
    "     with tf.Graph().as_default():\n",
    "        model = config.model\n",
    "        model.build(config.hparams,\n",
    "                    config.data_converter.output_depth,\n",
    "                    is_training=False)\n",
    "\n",
    "        eval_op = model.eval(**_get_input_tensors(dataset_fn().take(num_batches), config))\n",
    "\n",
    "        hooks = [\n",
    "            tf_slim.evaluation.StopAfterNEvalsHook(num_batches),\n",
    "            tf_slim.evaluation.SummaryAtEndHook(eval_dir)\n",
    "            ]\n",
    "\n",
    "        tf_slim.evaluation.evaluate_repeatedly(\n",
    "            train_dir,\n",
    "            eval_ops=eval_op,\n",
    "            hooks=hooks,\n",
    "            eval_interval_secs=60,\n",
    "            master=master)\n",
    "\n",
    "# 학습 실행 함수 정의\n",
    "def run(config_map,\n",
    "        tf_file_reader=tf.data.TFRecordDataset,\n",
    "        file_reader=tf.python_io.tf_record_iterator,\n",
    "        is_training=True):\n",
    "    config = config_map['drum_4bar']\n",
    "    train_dir = 'D:/PozaLabs_test/VAE/one_midi_train'\n",
    "    num_steps = 5000 #훈련 epoch\n",
    "    \n",
    "    def dataset_fn():\n",
    "        return data.get_dataset(\n",
    "            config,\n",
    "            tf_file_reader=tf_file_reader,\n",
    "            is_training=True,\n",
    "            cache_dataset=True)\n",
    "    \n",
    "    if is_training == True:\n",
    "        train(train_dir, config= config, dataset_fn= dataset_fn, num_steps= num_steps)      \n",
    "    \n",
    "    else:\n",
    "        print(\"EVAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 64, 'z_size': 512, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [1024, 1024], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [1024, 1024]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Reading examples from file: D:/PozaLabs/VAE/midi_data/m.tfrecord\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  mu = tf.layers.dense(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  sigma = tf.layers.dense(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:/PozaLabs/VAE/one_midi_train\\model.ckpt-80\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\saver.py:1175: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 80...\n",
      "INFO:tensorflow:Saving checkpoints for 80 into D:/PozaLabs/VAE/one_midi_train\\model.ckpt.\n",
      "INFO:tensorflow:D:/PozaLabs/VAE/one_midi_train\\model.ckpt-80.data-00000-of-00001\n",
      "INFO:tensorflow:661600\n",
      "INFO:tensorflow:D:/PozaLabs/VAE/one_midi_train\\model.ckpt-80.index\n",
      "INFO:tensorflow:661600\n",
      "INFO:tensorflow:D:/PozaLabs/VAE/one_midi_train\\model.ckpt-80.meta\n",
      "INFO:tensorflow:664300\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 80...\n",
      "INFO:tensorflow:global_step = 80, loss = 131.00009\n"
     ]
    }
   ],
   "source": [
    "# model train fucntion 수행\n",
    "\n",
    "run(CONFIG_MAP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 64, 'z_size': 512, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [1024, 1024], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [1024, 1024]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.layers.dense(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\contrib\\rnn.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "INFO:tensorflow:Restoring parameters from D:/PozaLabs/VAE/one_midi_train\\model.ckpt-80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  mu = tf.layers.dense(\n",
      "C:\\Users\\SESE\\AppData\\Roaming\\Python\\Python38\\site-packages\\magenta\\models\\music_vae\\base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  sigma = tf.layers.dense(\n"
     ]
    }
   ],
   "source": [
    "# 훈련 모델을 통해 4마디에 해당하는 드럼 샘플 추출 & 해당 샘플 midi format으로 저장\n",
    "\n",
    "model= TrainedModel(config= CONFIG_MAP['drum_4bar'], batch_size= 1, checkpoint_dir_or_path= 'D:/PozaLabs_test/VAE/one_midi_train')\n",
    "\n",
    "generated_sequence= model.sample(n= 1, length= 64, temperature= 0.5)\n",
    "note_seq.sequence_proto_to_midi_file(generated_sequence[0], 'D:/PozaLabs_test/VAE/gen_midi_one/one_midi_drum_4bar.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
